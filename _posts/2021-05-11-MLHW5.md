---
layout: single
categories: Project
title: MachineLearning - Logistic Regression
use_math: true
---

# 기본 설정


```python
# 공통 모듈 임포트
import numpy as np
import os

# 노트북 실행 결과를 동일하게 유지하기 위해
np.random.seed(2042)

# 깔끔한 그래프 출력을 위해
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# 데이터 준비
from sklearn import datasets
iris = datasets.load_iris()
```

# 과제 1 

조기 종료를 사용한 배치 경사 하강법으로 로지스틱 회귀를 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

__단계 1: 데이터 준비__ 

붓꽃 데이터셋의 꽃잎 길이(petal length)와 꽃잎 너비(petal width) 특성만 이용한다.


```python
X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이
y = (iris["target"] == 2).astype(np.int)
```

모든 샘플에 편향을 추가한다.


```python
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

결과를 일정하게 유지하기 위해 랜덤 시드를 지정합니다.


```python
np.random.seed(2042)
```

__단계 2: 데이터셋 분할__ 

데이터셋을 훈련, 검증, 테스트 용도로 6대 2대 2의 비율로 무작위로 분할한다.

- 훈련 세트: 60%
- 검증 세트: 20%
- 테스트 세트: 20%

아래 코드는 사이킷런의 `train_test_split()` 함수를 사용하지 않고 
수동으로 무작위 분할하는 방법을 보여준다.
먼저 각 세트의 크기를 결정한다.


```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```

`np.random.permutation()` 함수를 이용하여 인덱스를 무작위로 섞는다. 


```python
rnd_indices = np.random.permutation(total_size)
```

인덱스가 무작위로 섞였기 때문에 무작위로 분할하는 효과를 얻는다.
방법은 섞인 인덱스를 이용하여 지정된 6:2:2의 비율로 훈련, 검증, 테스트 세트로 분할하는 것이다.


```python
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```

샘플 5개에 대해 잘 작동하는 것을 확인할 수 있다.


```python
y_train[:5]
```




    array([0, 0, 1, 0, 0])



__단계 3: 로지스틱 함수 구현__ 

로지스틱 함수를 파이썬 함수로 구현한다. 

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

__단계 4: 경사하강법 활용 훈련__ 

경사하강법을 구현하기 위해 아래 비용함수와 비용함수의 그레이디언트를 파이썬으로 
구현할 수 있어야 한다.

로지스틱 함수를 이용해 모델을 훈련 시킬 때는 비용함수(로그손실) 함수를 사용한다.


```python
n_inputs = X_train.shape[1]
```

파라미터 Theta를 무작위로 초기 설정한다.


```python
Theta = np.random.randn(n_inputs)
```

배치 경사하강법 훈련은 아래 코드를 통해 이루어진다.

- `eta = 0.01`: 학습률
- `n_iterations = 5001` : 에포크 수
- `m = len(X_train)`: 훈련 세트 크기, 즉 훈련 샘플 수
- `epsilon = 1e-7`: log 값이 항상 계산되도록 더해지는 작은 실수
- `logits`: 모든 샘플에 대한 클래스별 점수
- `Y_proba`: 모든 샘플에 대해 계산된 클래스 별 소속 확률


```python
# 배치 경사하강법 구현
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta)
    Y_proba = sigmoid(logits)
   
    if iteration % 500 == 0:
        loss = -np.mean(np.sum((y_train*np.log(Y_proba + epsilon) + (1-y_train)*np.log(1 - Y_proba + epsilon))))
        print(iteration, loss)
    
    error = Y_proba - y_train     # 그레이디언트 계산.
    gradients = 1/m * X_train.T.dot(error)
    
    Theta = Theta - eta * gradients
```

    0 79.35473984499612
    500 27.149524631560638
    1000 21.894389285779454
    1500 19.337773447717062
    2000 17.691444239326714
    2500 16.49516908325313
    3000 15.566000472955372
    3500 14.813273989795578
    4000 14.185530546071131
    4500 13.65075154805576
    5000 13.187653637231026
    

학습된 파라미터는 다음과 같다.


```python
Theta
```




    array([-10.56492618,   0.53611169,   4.82694082])



검증 세트에 대한 예측과 정확도는 다음과 같다.
`logits`, `Y_proba`를 검증 세트인 `X_valid`를 이용하여 계산한다.
예측 클래스는 `Y_proba`에서 가장 큰 값을 갖는 인덱스로 선택한다.


```python
logits = X_valid.dot(Theta)              
Y_proba = sigmoid(logits)
y_predict = np.array([])

for i in range(len(Y_proba)):
    if Y_proba[i] >= 0.5:
        y_predict = np.append(y_predict, 1)
    else:
        y_predict = np.append(y_predict, 0)
        
accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    0.9666666666666667




```python
y_predict
```




    array([0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
           0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])




```python
y_valid
```




    array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
           1, 1, 0, 0, 0, 0, 0, 1])



__단계 5: 규제가 추가된 경사하강법 활용 훈련__ 

l2 규제가 추가된 경사하강법 훈련을 구현한다. 
코드는 기본적으로 동일하다.
다만 손실(비용)에 l2 페널티가 추가되었고 
그래디언트에도 항이 추가되었다(`Theta`의 첫 번째 원소는 편향이므로 규제하지 않습니다). 

- 학습률 `eta` 증가됨.
- `alpha = 0.1`: 규제 강도


```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum((y_train*np.log(Y_proba + epsilon) + (1-y_train)*np.log(1 - Y_proba + epsilon))))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - y_train
    l2_loss_gradients = np.r_[np.zeros([1]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```

    0 156.73838246234882
    500 36.11974638424874
    1000 34.306068180110614
    1500 34.02211206089248
    2000 33.9713877223945
    2500 33.96211929178583
    3000 33.96041878356459
    3500 33.960106551185575
    4000 33.96004921390298
    4500 33.96003868441418
    5000 33.96003675075696
    

검증 세트를 이용해 성능을 확인하니 점수가 조금 떨어졌다.


```python
logits = X_valid.dot(Theta)              
Y_proba = sigmoid(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
    if Y_proba[i] >= 0.5:
        y_predict = np.append(y_predict, 1)
    else:
        y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    0.8333333333333334



__단계 6: 조기 종료 추가__

위 규제가 사용된 모델의 훈련 과정에서
매 에포크마다 검증 세트에 대한 손실을 계산하여 오차가 줄어들다가 증가하기 시작할 때 멈추도록 한다.


```python
eta = 0.1 
n_iterations = 50000
m = len(X_train)
epsilon = 1e-7
alpha = 0.5            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta)
    Y_proba = sigmoid(logits)
    error = Y_proba - y_train
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta)
    Y_proba = sigmoid(logits)
    xentropy_loss = -np.mean(np.sum((y_valid*np.log(Y_proba + epsilon) + (1-y_valid)*np.log(1 - Y_proba + epsilon))))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되기 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 31.86188582266571
    500 12.54052343500867
    1000 11.955093912404864
    1500 11.865469014733975
    2000 11.849523658203214
    2500 11.8466122603015
    3000 11.846078169747395
    3500 11.845980107187028
    4000 11.84596209939774
    4500 11.845958792428052
    5000 11.84595818512936
    5500 11.845958073603672
    6000 11.84595805312284
    6500 11.845958049361693
    7000 11.845958048670985
    7500 11.845958048544144
    8000 11.845958048520849
    8336 11.845958048517291
    8337 11.845958048517291 조기 종료!
    

__단계 7: 테스트 세트 평가__

마지막으로 테스트 세트에 대한 모델의 최종 성능을 정확도로 측정한다.


```python
logits = X_test.dot(Theta)              
Y_proba = sigmoid(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
    if Y_proba[i] >= 0.5:
        y_predict = np.append(y_predict, 1)
    else:
        y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_test)  # 정확도 계산
accuracy_score
```




    0.9666666666666667



실제 모델과 유사한 성능이다.

# 과제 2

과제 1에서 구현된 로지스틱 회귀 알고리즘에 일대다(OvR) 방식을 적용하여 붓꽃에 대한 다중 클래스 분류 알고리즘을 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

2개의 로지스틱 모델을 사용한다.
- setosa인지 아닌지 판단하는 모델
- virginica인지 아닌지 판단하는 모델

그 후에 versicolor일 확률을 1-(setosa일 확률)-(virginica일 확률)로 계산할 수 있다.

**1.데이터 준비**

- `y0` : setosa 판단 모델을 위한 데이터셋
- `y1` : virginica 판단 모델을 위한 데이터셋


```python
X = iris["data"][:, (2, 3)] # 꽃잎의 길이, 꽃잎의 너비
y = iris["target"]
y0 = (iris["target"] == 0).astype(np.int) 
y1 = (iris["target"] == 2).astype(np.int) 
```


```python
# 편향 추가
X_with_bias = np.c_[np.ones([len(X), 1]), X] 
```

일정한 결과를 위해 랜덤시드를 지정한다.


```python
np.random.seed(2042)
```

**2. 데이터셋 분할**
- 테스트 세트 크기 : 20%
- 검증 세트 크기 : 20%
- 훈련 세트 크기 : 60%

데이터셋의 분할 크기를 정하고 무작위로 섞는다.


```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```


```python
rnd_indices = np.random.permutation(total_size) # 데이터 섞기
```

무작위로 섞은 데이터셋을 분할한다.


```python
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]
y_train0 = y0[rnd_indices[:train_size]] # setosa에 대한 훈련세트 라벨
y_train1 = y1[rnd_indices[:train_size]] # virginica에 대한 훈련세트 라벨

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]
y_valid0 = y0[rnd_indices[train_size:-test_size]] # setosa에 대한 검증세트 라벨
y_valid1 = y1[rnd_indices[train_size:-test_size]] # virginica에 대한 검증세트 라벨

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```

**3. setosa 판별 로지스틱 회귀 모델**


setosa 판단 모델에 쓰이는 세타 값을 무작위로 지정한다.


```python
n_inputs = X_train.shape[1]
```


```python
Theta0 = np.random.randn(n_inputs)
```


```python
# setosa 판별 로지스틱 회귀 모델 구현
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5           # 규제 하이퍼파라미터
best_loss0 = np.infty # 최소 손실값 기억 변수

Theta0 = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits0 = X_train.dot(Theta0)
    Y_proba0 = sigmoid(logits0)
    error = Y_proba0 - y_train0
    gradients0 = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta0[1:]]
    Theta0 = Theta0 - eta * gradients0

    # 검증 세트에 대한 손실 계산
    logits0 = X_valid.dot(Theta0)
    Y_proba0 = sigmoid(logits0)
    xentropy_loss0 = -np.mean(np.sum((y_valid0 * np.log(Y_proba0 + epsilon) 
                                      + (1 - y_valid0) * np.log(1 - Y_proba0 + epsilon))))
    l2_loss0 = 1/2 * np.sum(np.square(Theta0[1:]))
    loss0 = xentropy_loss0 + alpha * l2_loss0
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss0)
    
    # 에포크마다 최소 손실값 업데이트
    if loss0 < best_loss0:
        best_los0 = loss0
    # 에포크가 줄어들지 않으면 바로 훈련 종료
    else:
        print(iteration - 1, best_loss0) # 종료 전 에포크의 손실 값 출력
        print(iteration, loss0, "조기 종료!")
        break
```

    0 9.952536073264003
    500 7.771068352251202
    1000 7.674555771477751
    1500 7.668691596899765
    2000 7.668320548904073
    2500 7.668297010375545
    3000 7.6682955168934726
    3500 7.668295422133431
    4000 7.66829541612099
    4500 7.66829541573951
    5000 7.668295415715302
    

**4. virginica 판별 로지스틱 회귀 모델**

virginica 판단 모델에 쓰이는 세타 값을 무작위로 지정한다.


```python
Theta1 = np.random.randn(n_inputs)
```


```python
# virginica 판별 로지스틱 회귀 모델 구현
eta = 0.1
n_iterations = 5001
m = len(X_train)
esilon = 1e-7
alpha = 0.5           # 규제 하이퍼파라미터
best_loss1 = np.infty # 최소 손실값 기억 변수

Theta1 = np.random.randn(n_inputs) # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits1 = X_train.dot(Theta1)
    Y_proba1 = sigmoid(logits1)
    error = Y_proba1 - y_train1
    gradients1 = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta1[1:]]
    Theta1 = Theta1 - eta * gradients1
    
    # 검증 세트에 대한 손실 계산
    logits1 = X_valid.dot(Theta1)
    Y_proba1 = sigmoid(logits1)
    xentropy_loss1 = -np.mean(np.sum((y_valid1 * np.log(Y_proba1 + epsilon) 
                                      + (1 - y_valid1) * np.log(1 - Y_proba1 + epsilon))))
    l2_loss1 = 1/2 * np.sum(np.square(Theta1[1:]))
    loss1 = xentropy_loss1 + alpha * l2_loss1
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss1)
        
    # 에포크마다 최소 손실값 업데이트
    if loss1 < best_loss1:
        best_loss1 = loss1
    # 에포크가 줄어들지 않으면 바로 훈련 종료
    else:
        print(iteration - 1, best_loss1)
        print(iteration, loss1, "조기 종료!")
        break
```

    0 45.3881848638996
    500 12.482904005693054
    1000 11.947222069327108
    1500 11.864096195806567
    2000 11.849273910674974
    2500 11.846566475123907
    3000 11.846069764314986
    3500 11.845978563684064
    4000 11.845961815948371
    4500 11.845958740374874
    5000 11.845958175570196
    

**5. 테스트셋에 적용**

- setosa일 확률(`setosa_proba`)
- virginica일 확률(`virginica_proba`)
- versicolor일 확률(1 - `setosa_proba` - `virginica_proba`)

셋 중 가장 높은 것을 예측값으로 사용한다.


```python
logits = X_test.dot(Theta0) # setosa에 대한 확률값 추정
setosa_proba = sigmoid(logits)

logits = X_test.dot(Theta1) # virginica에 대한 확률값 추정
virginica_proba = sigmoid(logits)

y_predict = np.array([])
for i in range(len(Y_proba0)):
    prob_list = [[setosa_proba[i], 0],
                 [1-setosa_proba[i]-virginica_proba[i], 1],
                 [virginica_proba[i], 2]]
    prob_list.sort(reverse=True) # 가장 높은 확률이 맨 앞으로 오도록 정렬
    
    # 확률이 가장 높은 것을 예측값으로 사용
    y_predict = np.append(y_predict, prob_list[0][1]) 
```

정확도를 측정한다.


```python
accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.9333333333333333



**6.사이킷런 로지스틱 모델과의 성능 비교**

모델의 `solver`를 `newton-cg`로 하면 multinomial logistic regression 모델을 사용할 수 있다.


```python
from sklearn.linear_model import LogisticRegression
multi_log_reg = LogisticRegression(solver='newton-cg', random_state=42).fit(X_train, y_train)

multi_log_reg.score(X_test, y_test)
```

    C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.
      "this warning.", FutureWarning)
    




    0.9333333333333333



직접 구현한 코드와 사이킷런에 내장된 로지스틱 모델의 성능이 같음을 확인할 수 있었다.

 # 과제 3

A에서 직접 구현한 모델과 사이킷런의 로지스틱 모델의 성능 비교를 위해 A, C, B 순서로 진행힌다.

A. 사진을 낮과 밤으로 분류하는 로지스틱 회귀 모델을 구현하라.

**1. 이미지 다운로드**


```python
from urllib import request
url = "https://docs.google.com/uc?export=download&id=1emB4lSxEzxzEt7_20w2DZ_Dw1sYS1grA"
request.urlretrieve(url,"day_night.zip")
```




    ('day_night.zip', <http.client.HTTPMessage at 0x13ed0c30978>)




```python
# 압축 해제
import os
import zipfile

local_zip = 'C:/Users/gram/Downloads/day_night.zip'

zip_ref = zipfile.ZipFile(local_zip, 'r')

zip_ref.extractall('C:/Users/gram/Downloads')
zip_ref.close()
```


```python
# 모듈 준비
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import cv2
import os 
from random import shuffle 
from tqdm import tqdm 
from PIL import Image
import warnings
warnings.filterwarnings('ignore')
```

라벨과 train, test에 따라 경로를 지정한다.


```python
train_day = "day_night/train/day"
train_night = "day_night/train/night"
test_day = "day_night/test/day"
test_night = "day_night/test/night"

image_size = 128
```

확인을 위해 이미지를 불러온다.


```python
Image.open("day_night/train/day/day_120.jpg")
```




![output_80_0](https://user-images.githubusercontent.com/59548168/117808109-7359c180-b297-11eb-911c-39c3f6916284.png)




```python
Image.open("day_night/train/night/night_120.jpg")
```




![output_81_0](https://user-images.githubusercontent.com/59548168/117808114-75238500-b297-11eb-85bd-83ca6dd400f7.png)



**2. 데이터 전처리**
- resize
- 데이터 라벨링

사진의 크기에 따라 특성 수를 다르게 받아들이므로 이를 조정해주는 작업이 필요하다.

이를 resize라고 한다.


```python
# resize
for image in tqdm(os.listdir(train_night)): 
    path = os.path.join(train_night, image)
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img = cv2.resize(img, (image_size, image_size)).flatten()   
    np_img=np.asarray(img)
    
for image2 in tqdm(os.listdir(train_day)): 
    path = os.path.join(train_day, image2)
    img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img2 = cv2.resize(img2, (image_size, image_size)).flatten() 
    np_img2=np.asarray(img2)

plt.figure(figsize=(10,10))
plt.subplot(1, 2, 1)
plt.imshow(np_img.reshape(image_size, image_size))
plt.axis('off')
plt.subplot(1, 2, 2)
plt.imshow(np_img2.reshape(image_size, image_size))
plt.axis('off')
plt.title("day and night in GrayScale")
```

    100%|██████████| 400/400 [00:04<00:00, 98.35it/s] 
    100%|██████████| 400/400 [00:05<00:00, 73.15it/s]
    




    Text(0.5, 1.0, 'day and night in GrayScale')




![output_84_2](https://user-images.githubusercontent.com/59548168/117808115-75bc1b80-b297-11eb-9511-b54660823fbc.png)


경로 별로 나뉘어져 있는 낮과 밤 사진들을 하나의 트레이닝셋으로 합쳐 데이터를 라벨링한다.


```python
def train_data():
    train_data_night = [] 
    train_data_day=[]
    for image1 in tqdm(os.listdir(train_night)): 
        path = os.path.join(train_night, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        train_data_night.append(img1) 
    for image2 in tqdm(os.listdir(train_day)): 
        path = os.path.join(train_day, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        train_data_day.append(img2) 
    
    train_data= np.concatenate((np.asarray(train_data_night),
                                np.asarray(train_data_day)),axis=0)
    return train_data 
```

테스트셋에 대해 같은 과정을 반복한다.


```python
def test_data():
    test_data_night = [] 
    test_data_day=[]
    for image1 in tqdm(os.listdir(test_night)): 
        path = os.path.join(test_night, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        test_data_night.append(img1) 
    for image2 in tqdm(os.listdir(test_day)): 
        path = os.path.join(test_day, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        test_data_day.append(img2) 
    
    test_data= np.concatenate((np.asarray(test_data_night),
                               np.asarray(test_data_day)),axis=0) 
    return test_data 
```

**3. 훈련세트와 테스트세트 설정**

features와 label을 분리하여 저장한다.


```python
train_data = train_data() 
test_data = test_data()
```

    100%|██████████| 400/400 [00:00<00:00, 511.33it/s]
    100%|██████████| 400/400 [00:00<00:00, 527.79it/s]
    100%|██████████| 100/100 [00:01<00:00, 70.29it/s]
    100%|██████████| 100/100 [00:01<00:00, 66.05it/s]
    


```python
x_data=np.concatenate((train_data,test_data),axis=0)
x_data = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))
```


```python
z1 = np.zeros(400)
o1 = np.ones(400)
Y_train = np.concatenate((o1, z1), axis=0)
z = np.zeros(100)
o = np.ones(100)
Y_test = np.concatenate((o, z), axis=0)
```


```python
y_data=np.concatenate((Y_train,Y_test),axis=0).reshape(x_data.shape[0],1)
```


```python
print("X shape: " , x_data.shape)
print("Y shape: " , y_data.shape)
```

    X shape:  (1000, 128, 128)
    Y shape:  (1000, 1)
    

사이킷런의 `train_test_split`을 활용하여 훈련세트와 테스트셋을 분리한다.


```python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, 
                                                    test_size=0.15, random_state=42)
number_of_train = x_train.shape[0]
number_of_test = x_test.shape[0]
```


```python
x_train_flatten = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])
x_test_flatten = x_test.reshape(number_of_test,x_test.shape[1]*x_test.shape[2])
print("X train flatten",x_train_flatten.shape)
print("X test flatten",x_test_flatten.shape)
```

    X train flatten (850, 16384)
    X test flatten (150, 16384)
    


```python
x_train = x_train_flatten.T
x_test = x_test_flatten.T
y_test = y_test.T
y_train = y_train.T
day_night_y_test = y_test
print("x train: ",x_train.shape)
print("x test: ",x_test.shape)
print("y train: ",y_train.shape)
print("y test: ",y_test.shape)
```

    x train:  (16384, 850)
    x test:  (16384, 150)
    y train:  (1, 850)
    y test:  (1, 150)
    

**4. 로지스틱 모델 구현하기**


```python
def initialize_weights_and_bias(dimension):
    w = np.full((dimension,1),0.01)
    b = 0.0
    return w, b

def sigmoid(z):
    y_head = 1/(1+np.exp(-z))
    return y_head

def forward_backward_propagation(w,b,x_train,y_train):
    # forward propagation
    z = np.dot(w.T,x_train) + b
    y_head = sigmoid(z)
    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)
    cost = (np.sum(loss))/x_train.shape[1]
    # backward propagation
    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]
    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]
    gradients = {"derivative_weight": derivative_weight,"derivative_bias": derivative_bias}
    return cost,gradients

def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):
    cost_list = []
    cost_list2 = []
    index = []
    
    for i in range(number_of_iterarion):
        
        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)
        cost_list.append(cost)
        
        w = w - learning_rate * gradients["derivative_weight"]
        b = b - learning_rate * gradients["derivative_bias"]
        if i % 100 == 0:
            cost_list2.append(cost)
            index.append(i)
            print ("Cost after iteration %i: %f" %(i, cost))
    
    parameters = {"weight": w,"bias": b}
    plt.plot(index,cost_list2)
    plt.xticks(index,rotation='vertical')
    plt.xlabel("Number of Iterarion")
    plt.ylabel("Cost")
    plt.show()
    return parameters, gradients, cost_list

def predict(w,b,x_test):
    
    z = sigmoid(np.dot(w.T,x_test)+b)
    Y_prediction = np.zeros((1,x_test.shape[1]))

    for i in range(z.shape[1]):
        if z[0,i] <= 0.5:
            Y_prediction[0,i] = 0
        else:
            Y_prediction[0,i] = 1

    return Y_prediction

def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):

    dimension =  x_train.shape[0]
    w,b = initialize_weights_and_bias(dimension)

    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)
    
    y_prediction_test = predict(parameters["weight"],parameters["bias"],x_test)
    y_prediction_train = predict(parameters["weight"],parameters["bias"],x_train)
    
    print("Test Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))
    print("Train Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))
```

**5. 훈련**

에포크는 1500, 학습률을 0.01로 지정하고 학습을 시작한다.


```python
logistic_regression(x_train, y_train, x_test, y_test,
                    learning_rate = 0.01, num_iterations = 1500)
```

    Cost after iteration 0: nan
    Cost after iteration 100: 0.988575
    Cost after iteration 200: 0.724461
    Cost after iteration 300: 0.606940
    Cost after iteration 400: 0.576481
    Cost after iteration 500: 0.504360
    Cost after iteration 600: 0.426912
    Cost after iteration 700: 0.380144
    Cost after iteration 800: 0.349735
    Cost after iteration 900: 0.321913
    Cost after iteration 1000: 0.296174
    Cost after iteration 1100: 0.272192
    Cost after iteration 1200: 0.249787
    Cost after iteration 1300: 0.228887
    Cost after iteration 1400: 0.209464
    


![output_102_1](https://user-images.githubusercontent.com/59548168/117808118-75bc1b80-b297-11eb-9be9-27198aa42e1b.png)


    Test Accuracy: 76.0 %
    Train Accuracy: 96.0 %
    

train_set에 비해 test_set에 대해 성능이 낮게 나왔으므로 과대적합이 의심된다.

C. 과제 1에서 구현한 자신의 알고리즘과 사이킷런에서 제공하는 LogisticRegression 모델의 성능을 비교하라.

**1. 데이터 크기 조절하기**

사이킷런의 LogisticRegression을 사용하기 위해 데이터의 형태를 조정한다.


```python
x_train.shape
```




    (16384, 850)




```python
y_train.shape
```




    (1, 850)




```python
y_train2 = np.array([])
for i in y_train:
    y_train2 = np.append(y_train2, np.array([i]))
```


```python
y_test2 = np.array([])
for i in y_test:
    y_test2 = np.append(y_test2, np.array([i]))
```

잘 되었는지 확인한다.


```python
y_train2.shape
```




    (850,)



**2. LogisticRegression 활용**

- `solver` : saga
- `multi_class` : multinomial


```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty = 'none', tol=0.1, solver='saga',
                        multi_class='multinomial').fit(x_train.T, y_train2)
```


```python
clf.score(x_test.T, y_test2)
```




    0.7733333333333333




```python
pred1 = clf.predict(x_test.T)
```

직접 구현한 로지스틱 모델의 정확도와 사이킷런에 내장된 로지스틱 모델의 정확도는 유사하나, 사이킷런의 LogisticRegresiion 모델이 좀 더 우수했다.

B. 사진을 낮과 밤, 실내와 실외로 분류하는 다중 레이블 분류 모델을 두 개의 로지스틱 회귀 모델을 이용하여 구현하라.

낮과 밤을 분류하는 모델은 이미 위에서 구현하였으므로 실내와 실외를 분류하는 로지스틱 모델을 만든다.

**1. 이미지 다운로드**


```python
from urllib import request
url = "https://docs.google.com/uc?export=download&id=1CPbsXHOxFEAic3YQBxDdTDKEZXkXMdP1"
request.urlretrieve(url,"indoor_outdoor.zip")
```




    ('indoor_outdoor.zip', <http.client.HTTPMessage at 0x13ed09f8978>)




```python
# 압축 풀기
import os
import zipfile

local_zip = 'C:/Users/gram/Downloads/indoor_outdoor.zip'

zip_ref = zipfile.ZipFile(local_zip, 'r')

zip_ref.extractall('C:/Users/gram/Downloads')
zip_ref.close()
```


```python
# 경로 지정
train_indoor = "indoor_outdoor/train/indoors"
train_outdoor= "indoor_outdoor/train/outdoors"
test_indoor= "indoor_outdoor/test/indoors"
test_outdoor= "indoor_outdoor/test/outdoors"

image_size = 128
```

이미지가 잘 다운로드 되었는지 확인


```python
Image.open("indoor_outdoor/train/indoors/indoors.101.jpg")
```




![output_124_0](https://user-images.githubusercontent.com/59548168/117808119-7654b200-b297-11eb-8a52-5532d7405760.png)




```python
Image.open("indoor_outdoor/train/outdoors/outdoors_120.jpg")
```




![output_125_0](https://user-images.githubusercontent.com/59548168/117808120-76ed4880-b297-11eb-9e9a-19f0022529e1.png)



**2. 데이터 전처리**
- resize
- 데이터 라벨링


```python
# resize
for image in tqdm(os.listdir(train_indoor)): 
    path = os.path.join(train_indoor, image)
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img = cv2.resize(img, (image_size, image_size)).flatten()   
    np_img = np.asarray(img)
    
for image2 in tqdm(os.listdir(train_outdoor)): 
    path = os.path.join(train_outdoor, image2)
    img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img2 = cv2.resize(img2, (image_size, image_size)).flatten() 
    np_img2 = np.asarray(img2)

plt.figure(figsize=(10,10))
plt.subplot(1, 2, 1)
plt.imshow(np_img.reshape(image_size, image_size))
plt.axis('off')
plt.subplot(1, 2, 2)
plt.imshow(np_img2.reshape(image_size, image_size))
plt.axis('off')
plt.title("indoor and outdoor in GrayScale")
```

    100%|██████████| 400/400 [00:05<00:00, 70.54it/s]
    100%|██████████| 400/400 [00:05<00:00, 74.67it/s]
    




    Text(0.5, 1.0, 'indoor and outdoor in GrayScale')



![output_127_2](https://user-images.githubusercontent.com/59548168/117808121-7785df00-b297-11eb-9eb5-149107f6df17.png)


훈련세트와 데이터세트 구성하기


```python
def train_data():
    train_data_indoor = [] 
    train_data_outdoor = []
    for image1 in tqdm(os.listdir(train_indoor)): 
        path = os.path.join(train_indoor, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        train_data_indoor.append(img1) 
    for image2 in tqdm(os.listdir(train_outdoor)): 
        path = os.path.join(train_outdoor, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        train_data_outdoor.append(img2) 
    
    train_data = np.concatenate((np.asarray(train_data_indoor),
                                np.asarray(train_data_outdoor)),axis=0)
    return train_data 
```


```python
def test_data():
    test_data_indoor = [] 
    test_data_outdoor = []
    for image1 in tqdm(os.listdir(test_indoor)): 
        path = os.path.join(test_indoor, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        test_data_indoor.append(img1) 
    for image2 in tqdm(os.listdir(test_outdoor)): 
        path = os.path.join(test_outdoor, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        test_data_outdoor.append(img2) 
    
    test_data = np.concatenate((np.asarray(test_data_indoor),
                               np.asarray(test_data_outdoor)),axis=0) 
    return test_data
```

**3. 훈련세트와 테스트세트 설정**


```python
train_data = train_data() 
test_data = test_data()
```

    100%|██████████| 400/400 [00:00<00:00, 492.86it/s]
    100%|██████████| 400/400 [00:00<00:00, 476.90it/s]
    100%|██████████| 100/100 [00:01<00:00, 67.91it/s]
    100%|██████████| 100/100 [00:01<00:00, 74.83it/s]
    


```python
x_data = np.concatenate((train_data,test_data),axis=0)
x_data = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))
```


```python
z1 = np.zeros(400)
o1 = np.ones(400)
Y_train = np.concatenate((o1, z1), axis=0)
z = np.zeros(100)
o = np.ones(100)
Y_test = np.concatenate((o, z), axis=0)
```


```python
y_data = np.concatenate((Y_train,Y_test),axis=0).reshape(x_data.shape[0],1)
```


```python
print("X shape: " , x_data.shape)
print("Y shape: " , y_data.shape)
```

    X shape:  (1000, 128, 128)
    Y shape:  (1000, 1)
    

사이킷런의 `train_test_split`을 활용하여 훈련세트와 테스트셋을 분리한다.


```python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=42)
number_of_train = x_train.shape[0]
number_of_test = x_test.shape[0]
```


```python
x_train_flatten = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])
x_test_flatten = x_test .reshape(number_of_test,x_test.shape[1]*x_test.shape[2])
print("X train flatten",x_train_flatten.shape)
print("X test flatten",x_test_flatten.shape)
```

    X train flatten (850, 16384)
    X test flatten (150, 16384)
    


```python
x_train = x_train_flatten.T
x_test = x_test_flatten.T
y_test = y_test.T
y_train = y_train.T
out_doors_y_test = y_test
print("x train: ",x_train.shape)
print("x test: ",x_test.shape)
print("y train: ",y_train.shape)
print("y test: ",y_test.shape)
```

    x train:  (16384, 850)
    x test:  (16384, 150)
    y train:  (1, 850)
    y test:  (1, 150)
    

**4. 로지스틱 모델 구현**


```python
def initialize_weights_and_bias(dimension):
    w = np.full((dimension,1),0.01)
    b = 0.0
    return w, b

def sigmoid(z):
    y_head = 1/(1+np.exp(-z))
    return y_head

def forward_backward_propagation(w,b,x_train,y_train):
    # forward propagation
    z = np.dot(w.T,x_train) + b
    y_head = sigmoid(z)
    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)
    cost = (np.sum(loss))/x_train.shape[1]
    # backward propagation
    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]
    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]
    gradients = {"derivative_weight": derivative_weight,"derivative_bias": derivative_bias}
    return cost,gradients

def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):
    cost_list = []
    cost_list2 = []
    index = []
    
    for i in range(number_of_iterarion):
        
        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)
        cost_list.append(cost)
        
        w = w - learning_rate * gradients["derivative_weight"]
        b = b - learning_rate * gradients["derivative_bias"]
        if i % 100 == 0:
            cost_list2.append(cost)
            index.append(i)
            print ("Cost after iteration %i: %f" %(i, cost))
    
    parameters = {"weight": w,"bias": b}
    plt.plot(index,cost_list2)
    plt.xticks(index,rotation='vertical')
    plt.xlabel("Number of Iterarion")
    plt.ylabel("Cost")
    plt.show()
    return parameters, gradients, cost_list

def predict(w,b,x_test):
    
    z = sigmoid(np.dot(w.T,x_test)+b)
    Y_prediction = np.zeros((1,x_test.shape[1]))

    for i in range(z.shape[1]):
        if z[0,i]<= 0.5:
            Y_prediction[0,i] = 0
        else:
            Y_prediction[0,i] = 1

    return Y_prediction

def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):

    dimension =  x_train.shape[0]
    w,b = initialize_weights_and_bias(dimension)

    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)
    
    y_prediction_test = predict(parameters["weight"],parameters["bias"],x_test)
    y_prediction_train = predict(parameters["weight"],parameters["bias"],x_train)
    
    print("Test Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))
    print("Train Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))
```

**5. 훈련**

에포크는 1500, 학습률을 0.01로 지정하고 학습을 시작한다.


```python
logistic_regression(x_train, y_train, x_test, y_test,
                    learning_rate = 0.01, num_iterations = 1500)
```

    Cost after iteration 0: nan
    Cost after iteration 100: 3.477332
    Cost after iteration 200: 3.488631
    Cost after iteration 300: 3.238884
    Cost after iteration 400: 3.111699
    Cost after iteration 500: 3.008554
    Cost after iteration 600: 2.912879
    Cost after iteration 700: 2.822541
    Cost after iteration 800: 2.736323
    Cost after iteration 900: 2.654448
    Cost after iteration 1000: 2.577169
    Cost after iteration 1100: 2.503902
    Cost after iteration 1200: 2.433777
    Cost after iteration 1300: 2.366150
    Cost after iteration 1400: 2.300616
    


![output_144_1](https://user-images.githubusercontent.com/59548168/117808126-7785df00-b297-11eb-9ab2-0326b48ff151.png)


    Test Accuracy: 53.33 %
    Train Accuracy: 64.35 %
    

결과가 좋지 않다.

**6. 사이킷런과 성능 비교**


```python
in_out_y_train = np.array([])
for i in y_train:
    in_out_y_train = np.append(in_out_y_train, np.array([i]))
```


```python
in_out_y_test = np.array([])
for i in y_test:
    in_out_y_test = np.append(in_out_y_test, np.array([i]))
```


```python
from sklearn.linear_model import LogisticRegression

lg2 = LogisticRegression(penalty='none', 
                         tol=0.1, solver='saga',C = 0.5,
                         multi_class='multinomial').fit(x_train.T, in_out_y_train)
```


```python
pred2 = lg2.predict(x_test.T)
```


```python
lg2.score(x_test.T, in_out_y_test)
```




    0.6066666666666667



직접 구현한 것보다 높게 나왔으나 여전히 좋지 않다.

**7. 낮과 밤, 실내와 실외 라벨 합치기**


준비된 두 모델의 예측값을 합쳐 하나의 array로 만들고 다중 라벨 분류를 완성한다.

낮과 밤에 대한 예측 결과와 실내와 실외에 대한 예측 결과를 샘플별로 묶어 리스트에 저장한다.


```python
multi_label_list = []
for i in range(len(pred1)):
    multi_label_list.append([pred1[i], pred2[i]]) 
```

저장된 리스트를 array로 바꾼다.


```python
multi_label_pred = np.array(multi_label_list)
```

낮과 밤, 실내와 실외 분류에 대한 정답을 샘플별로 묶어서 리스트에 저장한다.


```python
multi_label_test_list = []
for i in range(len(out_doors_y_test)):
    multi_label_test_list.append([day_night_y_test[0][i], out_doors_y_test[0][i]])
```

저장된 리스트를 array로 바꿔준다.


```python
multi_label_y_test = np.array(multi_label_test_list)
```

**8. 정확도 측정**


```python
accuracy_score = np.mean(multi_label_pred == multi_label_y_test)
accuracy_score
```




    0.5233333333333333



결과를 보아 다중 분류 모델의 성능이 좋지 않았음을 알 수 있다.

낮게 나온 이유는 학습에 필요한 데이터와 좋은 품질의 샘플이 부족했기 때문이라고 생각된다.
