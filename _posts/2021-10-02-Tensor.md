
# **< Tensor >**

Tensor는 uniform type을 가진 다차원 배열이다.   
NumPy의 array와 비슷하지만, Tensor는 배열 안의 요소 수정이 불가능하다는 것에 차이가 있다.   
아래와 같이 tensorflow 모듈을 import하여 사용할 수 있다.


```python
import tensorflow as tf
import numpy as np
```

## **1. Basics**

Tensor를 사용하기 위해 Rank 개념을 알 필요가 있다. Rank 뒤의 숫자는 축의 개수를 의미한다.

- Rank-0(**Scalar**) : 단일 값
- Rank-1(**Vector**) : 1개의 축을 포함
- Rank-2(**Matrix**) : 2개의 축을 포함(행렬)
- Rank-3 : 3개의 축을 포함(큐브, 차원(dimension))
- Rank-n : n개의 축을 포함







```python
# Rank-0 (단일 값)
rank_0_tensor = tf.constant(2)

# Rank-1 (벡터)
rank_1_tensor = tf.constant([2., 3., 5.])

# Rank-2 (행렬)
# 데이터 타입을 지정할 수 있음
rank_2_tensor = tf.constant([[1., 2.],
                            [3., 4.],
                            [5., 6.]], dtype=tf.float16)

# Rank-3 (큐브, 차원)
rank_3_tensor = tf.constant([[[0, 1, 2, 3, 4],
                            [5, 6, 7, 8, 9]],
                            [[10, 11, 12, 13, 14],
                            [15, 16, 17, 18, 19]],
                            [[20, 21, 22, 23, 24],
                            [25, 26, 27, 28, 29]],])

print("Rank-0: ", rank_0_tensor, "\n")
print("Rank-1: ", rank_1_tensor, "\n")
print("Rank-2: ", rank_2_tensor, "\n")
print("Rank-3: ", rank_3_tensor, "\n")
```

    Rank-0:  tf.Tensor(2, shape=(), dtype=int32) 
    
    Rank-1:  tf.Tensor([2. 3. 5.], shape=(3,), dtype=float32) 
    
    Rank-2:  tf.Tensor(
    [[1. 2.]
     [3. 4.]
     [5. 6.]], shape=(3, 2), dtype=float16) 
    
    Rank-3:  tf.Tensor(
    [[[ 0  1  2  3  4]
      [ 5  6  7  8  9]]
    
     [[10 11 12 13 14]
      [15 16 17 18 19]]
    
     [[20 21 22 23 24]
      [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32) 
    
    

Tensor를 **NumPy 배열로 변환**하기 위해 `np.array` 또는 `tensor.numpy` 메소드들을 사용한다.


```python
convert_np_array_1 = np.array(rank_2_tensor)

convert_np_array_2 = rank_2_tensor.numpy()

print(convert_np_array_1, type(convert_np_array_1))
print(convert_np_array_2, type(convert_np_array_2))
```

    [[1. 2.]
     [3. 4.]
     [5. 6.]] <class 'numpy.ndarray'>
    [[1. 2.]
     [3. 4.]
     [5. 6.]] <class 'numpy.ndarray'>
    


* Tensor는 주로 float형과 int형을 포함하지만 **복소수**와 **문자열** 같은 다양한 타입을 포함하고 있다.

참고 : **[Tensor의 데이터타입](https://www.tensorflow.org/api_docs/python/tf/dtypes)**


* 기본 `tf.Tensor` class는 축을 따라 모든 요소의 크기가 같은 "rectangular" 형태이지만, **Ragged tensors**와 **Sparse tensors**라는 특수한 유형이 있다.

**[Ragged tensors](https://www.tensorflow.org/guide/ragged_tensor)** : 균형하지 않은 모양의 Tensor로 데이터를 쉽게 저장하고 처리할 수 있다.


**[Sparse tensors](https://www.tensorflow.org/guide/sparse_tensor)** : values, indices, dense_shape 구성 형식을 사용하여 인코딩되는 Tensor로 임베딩과 같은 초희소 행렬에 최적화되어 있다.


* Tensor는 여러 연산에 사용할 수 있다.   
* 예를 들어, Tensor에 대해 **덧셈** (`tf.add` 또는 +), **요소별 곱셈**(`tf.multiply` 또는  \*), **행렬 곱셈**(`tf.matmul` 또는 @)을 할 수 있다.


참고 : **[Tensor의 연산 - tf.math](https://www.tensorflow.org/api_docs/python/tf/math)**


```python
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]])

# 덧셈
print(tf.add(a, b), "\n")
# 요소별 곱셈
print(tf.multiply(a, b), "\n")
# 행렬 곱셈
print(tf.matmul(a, b), "\n")

print(tf.add(a, b)==(a + b), "\n")
print(tf.multiply(a, b)==(a * b), "\n")
print(tf.matmul(a, b)==(a @ b), "\n")
```

    tf.Tensor(
    [[2 3]
     [4 5]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[1 2]
     [3 4]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[3 3]
     [7 7]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[ True  True]
     [ True  True]], shape=(2, 2), dtype=bool) 
    
    tf.Tensor(
    [[ True  True]
     [ True  True]], shape=(2, 2), dtype=bool) 
    
    tf.Tensor(
    [[ True  True]
     [ True  True]], shape=(2, 2), dtype=bool) 
    
    


```python
c = tf.constant([[4.0, 5.0], [10.0, 1.0]])

# 최댓값 찾기
print(tf.reduce_max(c))
# 가장 큰 값의 원소가 있는 인덱스 찾기
print(tf.argmax(c))
# 소프트맥스 계산
print(tf.nn.softmax(c))
```

    tf.Tensor(10.0, shape=(), dtype=float32)
    tf.Tensor([1 0], shape=(2,), dtype=int64)
    tf.Tensor(
    [[2.6894143e-01 7.3105860e-01]
     [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)
    

## **2. About shapes**

Tensors를 설명할 때 필요한 용어들이 존재한다.   
* **Shape** : Tensor의 각 차원의 길이(요소의 수)
* **Rank** : Tensor 축의 수
* **Axis** 또는 **Dimension** : Tensor의 특정 차원
* **Size** : Tensor의 총 항목 수, 곱 형태의 벡터

\* 참고 : "2차원 Tensor"를 참조할 수는 있지만 Rank-2가 일반적으로 2차원 공간을 의미한다고 할 수는 없다.

Tensors의 `tf.TensorShape` 객체로 편리하게 속성에 접근할 수 있다.   
아래와 같은 Rank-4의 Tensor가 있을 때, 축에 주의해야 한다.   
보통 global에서 local : Batch 축, 공간 크기 관련 축 (Width, Height 등), Features 축 순서를 가진다.   


```python
rank_4_tensor = tf.zeros([3, 2, 4, 5])

print("모든 요소의 데이터 타입:", rank_4_tensor.dtype)
print("축의 개수:", rank_4_tensor.ndim)
print("Tensor의 Shape:", rank_4_tensor.shape)
print("Tensor의 0번 축에 있는 요소 개수:", rank_4_tensor.shape[0])
print("Tensor의 마지막 축에 있는 요소 개수:", rank_4_tensor.shape[-1])
print("모든 요소 수: ", tf.size(rank_4_tensor).numpy())
```

    모든 요소의 데이터 타입: <dtype: 'float32'>
    축의 개수: 4
    Tensor의 Shape: (3, 2, 4, 5)
    Tensor의 0번 축에 있는 요소 개수: 3
    Tensor의 마지막 축에 있는 요소 개수: 5
    모든 요소 수:  120
    

## **3. indexing**

참고 : **[Tensor 슬라이싱 가이드](https://www.tensorflow.org/guide/tensor_slicing)**

### **3.1 Single-axis indexing**


파이썬 인덱싱 규칙과 NumPy 인덱싱의 규칙을 따른다.   
* 인덱스의 시작은 0부터이다.
* 음수 인덱스는 끝에서부터 거꾸로 계산한다.
* 콜론(:)은 `start:stop:step` 형식으로 사용한다.


```python
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())
```

    [ 0  1  1  2  3  5  8 13 21 34]
    

Scalar를 사용하여 인덱싱 : 축 제거


```python
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
```

    First: 0
    Second: 1
    Last: 34
    

콜론을 사용하여 인덱싱 : 축 유지


```python
print("모든 요소:", rank_1_tensor[:].numpy())
print("4번째 축 이전:", rank_1_tensor[:4].numpy())
print("4번째 축부터 끝까지:", rank_1_tensor[4:].numpy())
print("2번째 축부터 7번째 축 이전:", rank_1_tensor[2:7].numpy())
print("0부터 짝수번째 축:", rank_1_tensor[::2].numpy())
print("모든 요소 끝에서 부터:", rank_1_tensor[::-1].numpy())
```

    모든 요소: [ 0  1  1  2  3  5  8 13 21 34]
    4번째 축 이전: [0 1 1 2]
    4번째 축부터 끝까지: [ 3  5  8 13 21 34]
    2번째 축부터 7번째 축 이전: [1 2 3 5 8]
    0부터 짝수번째 축: [ 0  1  3  8 21]
    모든 요소 끝에서 부터: [34 21 13  8  5  3  2  1  1  0]
    

### **3.2 Multi-axis indexing**

높은 Rank의 Tensor는 여러 인덱스를 전달함으로써 인덱싱할 수 있다.   
Single-axis indexing과 같은 규칙이 각 축에 독립적으로 적용된다.


```python
# Rank-2 Tensor
print(rank_2_tensor.numpy())
```

    [[1. 2.]
     [3. 4.]
     [5. 6.]]
    

각 인덱스에 정수를 전달 : Scalar


```python
# Rank-2 Tensor에서 단일 값 가져오기
print(rank_2_tensor[1, 1].numpy())
```

    4.0
    


```python
# Tensor의 행과 열 가져오기
print("두번째 행:", rank_2_tensor[1, :].numpy())
print("두번째 열:", rank_2_tensor[:, 1].numpy())
print("마지막 행:", rank_2_tensor[-1, :].numpy())
print("마지막 열의 첫번째 요소:", rank_2_tensor[0, -1].numpy())
print("첫번째 행을 제외한 나머지 행:")
print(rank_2_tensor[1:, :].numpy())
```

    두번째 행: [3. 4.]
    두번째 열: [2. 4. 6.]
    마지막 행: [5. 6.]
    마지막 열의 첫번째 요소: 2.0
    첫번째 행을 제외한 나머지 행:
    [[3. 4.]
     [5. 6.]]
    


```python
# Rank-3 Tensor(3*2*5)에서 3번째 축의 5번째 열 가져오기
print(rank_3_tensor[:, :, 4])
```

    tf.Tensor(
    [[ 4  9]
     [14 19]
     [24 29]], shape=(3, 2), dtype=int32)
    

## **4. Manipulating Shapes**

Tensor의 모양을 바꾸는 것은 매우 유용하다.



```python
# `.shape`는 각 축에 따른 크기를 보여주는 `TensorShape`객체를 반환한다.
x = tf.constant([[1], [2], [3]])
print(x.shape)
```

    (3, 1)
    


```python
# 이 객체를 파이썬 리스트로도 변환할 수 있다.
print(x.shape.as_list())
```

    [3, 1]
    

`tf.reshape` 이용하여 Tensor를 새로운 Shape으로 바꿀 수 있다.


```python
# list 형태로 넘겨주어야 한다.
reshaped = tf.reshape(x, [1, 3])

print(x.shape)
print(reshaped.shape)
```

    (3, 1)
    (1, 3)
    

Tensor를 평평하게 만들면 어떤 순서로 메모리에 위치하는지 확인할 수 있다.


```python
# Shape에 넘겨진 `-1` 인자는 
print(tf.reshape(rank_3_tensor, [-1]))
```

    tf.Tensor(
    [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
     24 25 26 27 28 29], shape=(30,), dtype=int32)
    

일반적으로 `tf.reshape`는 인접한 축을 결합하거나 분할하는 것 (또는 1을 추가/제거) 뿐이다.   
3x2x5 Tensor는 슬라이스가 섞이지 않도록 (3x2)x5 또는 3x(2x5) 형태로 재구성하는 것이 좋다.


```python
print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
print(tf.reshape(rank_3_tensor, [3, -1]))
```

    tf.Tensor(
    [[ 0  1  2  3  4]
     [ 5  6  7  8  9]
     [10 11 12 13 14]
     [15 16 17 18 19]
     [20 21 22 23 24]
     [25 26 27 28 29]], shape=(6, 5), dtype=int32) 
    
    tf.Tensor(
    [[ 0  1  2  3  4  5  6  7  8  9]
     [10 11 12 13 14 15 16 17 18 19]
     [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)
    

재구성하는 것은 전체 요소 수가 같은, 어떤 새로운 Shape에 대해서도 작동하지만, 축의 순서를 고려하지 않으면 유용하게 사용할 수 없다.

`tf.reshape`에서 축들의 교환이 작동하지 않으면 `tf.transpose`를 해야한다.


```python
# 나쁜 예

# 모양을 바꾸면 축을 변경할 수 없다.
print(tf.reshape(rank_3_tensor, [2, 3, 5]), "\n") 

# 엉망인 형태
print(tf.reshape(rank_3_tensor, [5, 6]), "\n")

# 작동하지 않을 것
try:
  tf.reshape(rank_3_tensor, [7, -1])
except Exception as e:
  print(f"{type(e).__name__}: {e}")
```

    tf.Tensor(
    [[[ 0  1  2  3  4]
      [ 5  6  7  8  9]
      [10 11 12 13 14]]
    
     [[15 16 17 18 19]
      [20 21 22 23 24]
      [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) 
    
    tf.Tensor(
    [[ 0  1  2  3  4  5]
     [ 6  7  8  9 10 11]
     [12 13 14 15 16 17]
     [18 19 20 21 22 23]
     [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) 
    
    InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]
    

완전히 지정되지 않은 Shape에서 실행할 수 있다.   
* Shape에 `None`(축 길이를 알 수 없음)을 포함한다.
* 전체 Shape이 `None`(Tensor의 Rank를 알 수 없음)이다.

`tf.RaggedTensor`을 제외하고 이러한 Shpae은 TensorFlow의 graph-building API의 context에서만 발생한다.

* [tf.function](function.ipynb) 
* The [keras functional API](keras/functional.ipynb).

## **5. More on DTypes**

`tf.Tensor`의 데이터 타입을 검사하기 위해서는 `Tensor.dtype`을 사용한다.

* Python에서 `tf.Tensor`를 생성할 때 데이터 유형을 지정할 수 있다.

* 지정하지 않으면, TensorFlow에서는 데이터를 나타낼 수 있는 데이터 타입을 선택한다.   
(파이썬에서 정수는 `tf.int32`, 부동 소수점 숫자는 `tf.float32` 형태이다. 그렇지 않으면 NumPy가 배열로 변환 시 사용하는 것과 같은 방식을 사용한다.)

유형별로 cast 할 수 있다.


```python
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
# Now, let's cast to an uint8 and lose the decimal precision
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)
```

    tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
    

## **6. Broadcasting**

NumPy에서 빌린 개념으로, 특정한 조건에서 작은 Tensor가 큰 Tensor와 결합하는 연산을 할 때 자독적으로 "확장"되는 것이다.

가장 간단하고 일반적인 경우는 단일 값에 Tensor를 곱하거나 추가할 때이다.   
이 경우에 단일 값은 다른 인자와 같은 Shape로 Broadcasting된다.


```python
x = tf.constant([1, 2, 3])

y = tf.constant(2)
z = tf.constant([2, 2, 2])

# 아래의 계산 결과는 모두 같음
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
```

    tf.Tensor([2 4 6], shape=(3,), dtype=int32)
    tf.Tensor([2 4 6], shape=(3,), dtype=int32)
    tf.Tensor([2 4 6], shape=(3,), dtype=int32)
    

길이가 1인 축은 다른 인자와 일치하도록 확장될 수 있다.   
이 경우, 3x1 행렬에 요소별로 1x4 행렬을 곱하여 3x4 행렬을 만든다.


```python
# These are the same computations
x = tf.reshape(x,[3,1])
y = tf.range(1, 5) # [4]
print(x, "\n")
print(y, "\n")
print(tf.multiply(x, y))
```

    tf.Tensor(
    [[1]
     [2]
     [3]], shape=(3, 1), dtype=int32) 
    
    tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) 
    
    tf.Tensor(
    [[ 1  2  3  4]
     [ 2  4  6  8]
     [ 3  6  9 12]], shape=(3, 4), dtype=int32)
    

위 계산과 같은 결과를 갖는 Broadcasting을 사용하지 않은 경우이다.


```python
x_stretch = tf.constant([[1, 1, 1, 1],
                         [2, 2, 2, 2],
                         [3, 3, 3, 3]])

y_stretch = tf.constant([[1, 2, 3, 4],
                         [1, 2, 3, 4],
                         [1, 2, 3, 4]])

print(x_stretch * y_stretch)  # Again, operator overloading
```

    tf.Tensor(
    [[ 1  2  3  4]
     [ 2  4  6  8]
     [ 3  6  9 12]], shape=(3, 4), dtype=int32)
    

대부분 Broadcasting은 메모리에서 구체화되지 않으므로 시간적으로나 공간적으로나 효율적이지만,

`tf.broadcast_to`를 사용하여 Broadcasting이 되면 어떤 Shape인지 알 수 있다.


```python
print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))
```

    tf.Tensor(
    [[1 2 3]
     [1 2 3]
     [1 2 3]], shape=(3, 3), dtype=int32)
    

## **7. tf.convert_to_tensor**

`tf.matmul`와 `tf.reshape` 같은 대부분의 연산자는 `tf.Tensor` 클래스의 인자를 사용하지만, `tf.convert_to_tensor`는 Tensor Shape의 파이썬 객체가 허용된다.

전부는 아니지만 대부분 연산자는 Tensor가 아닌 인자에 대해 `convert_to_tensor`를 호출한다.    
변환 registry가 있어 NumPy의 `ndarray`, `TensorShape`, Python 목록 및 `tf.Variable`와 같은 대부분의 객체 클래스는 모두 자동으로 변환된다.

자신만의 유형이 있는 경우 Tensor로 자동 변환할 수 있다.

자세한 내용은 tf.register_tensor_conversion_function을 참조

## **8. Ragged Tensors**

어떤 축에 다양한 수의 요소를 가진 Tensor를 "Ragged"라고 한다.   

`tf.ragged.RaggedTensor`를 사용한다.




```python
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
```


```python
try:
  tensor = tf.constant(ragged_list)
except Exception as e:
  print(f"{type(e).__name__}: {e}")
```

    ValueError: Can't convert non-rectangular Python sequence to Tensor.
    

`tf.RaggedTensor` 대신 `tf.ragged.constant`를 사용해 만들 수도 있다.


```python
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
```

    <tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>
    

`tf.RaggedTensor`의 Shape에는 길이를 알 수 없는 축이 포함될 것이다.


```python
print(ragged_tensor.shape)
```

    (4, None)
    

## **9. String tensors**

`tf.string`은 데이터 타입이며, Tensor에서 문자열(가변 길이의 byte 배열)과 같은 데이터를 나타낼 수 있다.

문자열은 Python 문자열과 같이 인덱싱 할 수 없다. 문자열의 길이는 Tensor의 축이 아니다.

참고 : **[문자열 조작, `tf.strings`](https://www.tensorflow.org/api_docs/python/tf/strings)**

Scalar 문자열 Tensor


```python
scalar_string_tensor = tf.constant("Gray wolf")
print(scalar_string_tensor)
```

    tf.Tensor(b'Gray wolf', shape=(), dtype=string)
    

문자열 Vector : 문자열의 길이는 포함되지 않아 (3, )의 Shape을 가진다.


```python
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
print(tensor_of_strings)
```

    tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)
    

위의 출력에서 `b`는 데이터 타입이 바이트 문자열임을 나타내는 것이다.

유니코드 문자를 전달하면 UTF-8로 인코딩된다.


```python
tf.constant("🥳👍")
```




    <tf.Tensor: shape=(), dtype=string, numpy=b'\xf0\x9f\xa5\xb3\xf0\x9f\x91\x8d'>



 문자열의 일부 기본 함수는 `tf.strings.split`을 포함하여 `tf.strings`에서도 찾을 수 있다.

`tf.strings.split`를 사용하면, 문자열을 Tensor 세트로 분할할 수 있으나, 각 문자열이 다른 수의 여러 부분으로 나뉘어  RaggedTensor로 변한다.


```python
print(tf.strings.split(scalar_string_tensor, sep=" "), "\n")

print(tf.strings.split(tensor_of_strings))
```

    tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string) 
    
    <tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>
    

`tf.string.to_number`를 사용할 수 있다.


```python
text = tf.constant("1 10 100")
print(tf.strings.to_number(tf.strings.split(text, " ")))
```

    tf.Tensor([  1.  10. 100.], shape=(3,), dtype=float32)
    

`tf.cast`를 사용하면 문자열 Tenor를 숫자로 즉시 변환할 수 없지만, 바이트로 변경 후 숫자로 변환하면 된다.


```python
byte_strings = tf.strings.bytes_split(tf.constant("Duck"))
byte_ints = tf.io.decode_raw(tf.constant("Duck"), tf.uint8)
print("Byte strings:", byte_strings)
print("Bytes:", byte_ints)
```

    Byte strings: tf.Tensor([b'D' b'u' b'c' b'k'], shape=(4,), dtype=string)
    Bytes: tf.Tensor([ 68 117  99 107], shape=(4,), dtype=uint8)
    

분할하여 유니코드로 변환하고 해독하는 것도 하나의 방법이다.


```python
unicode_bytes = tf.constant("アヒル 🦆")
unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, "UTF-8")
unicode_values = tf.strings.unicode_decode(unicode_bytes, "UTF-8")

print("\nUnicode bytes:", unicode_bytes)
print("\nUnicode chars:", unicode_char_bytes)
print("\nUnicode values:", unicode_values)
```

    
    Unicode bytes: tf.Tensor(b'\xe3\x82\xa2\xe3\x83\x92\xe3\x83\xab \xf0\x9f\xa6\x86', shape=(), dtype=string)
    
    Unicode chars: tf.Tensor([b'\xe3\x82\xa2' b'\xe3\x83\x92' b'\xe3\x83\xab' b' ' b'\xf0\x9f\xa6\x86'], shape=(5,), dtype=string)
    
    Unicode values: tf.Tensor([ 12450  12498  12523     32 129414], shape=(5,), dtype=int32)
    

`tf.string` 데이터 타입은 TensorFlow의 모든 원시 바이트 데이터에 사용된다.   
`tf.io` 모듈에는 이미지 디코딩 및 csv 구문 분석을 포함하여 데이터를 바이트로 또는 바이트로부터 변환하는 함수가 포함되어 있다.

## **10. Spares tensors**




때때로 Tensor의 공간에 비해 데이터가 희소한 경우가 있다.

이때 `tf.sparse.SparseTensor`를 통해 희소 데이터를 효율적으로 저장할 수 있다.


```python
# Sparse Tensor는 인덱스별로 값을 저장하는 효율적인 메모리 방식을 사용한다.
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, "\n")

# Sparse Tensor를 고밀도로 변환할 수 있다.
print(tf.sparse.to_dense(sparse_tensor))
```

    SparseTensor(indices=tf.Tensor(
    [[0 0]
     [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) 
    
    tf.Tensor(
    [[1 0 0 0]
     [0 0 2 0]
     [0 0 0 0]], shape=(3, 4), dtype=int32)
    
